# Case-study оптимизации

## Актуальная проблема
При импорте данных в нашем проекте из json файла в базу возникает проблема.

Необходимо обработать файл размером более 30Mb.

Для мпорта данныз в настоящее время у нас есть rake таска, но на большом количестве данных она выполняется очень медленно.

Я решил исправить эту проблему, оптимизировав rake скрипт импорта данных.

### А Импорт данных
## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: Импорт файла large.json должен занимать менее 1 минуты.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений.

Вот как я построил `feedback_loop`:
Добавил в скрипт бенчмарк для измерения продолжительности импорта.

Вот как я построил `feedback_loop`:
- Запускаю имопрт данных.
- Запускаю профилировщик.
- Определяю главную точку роста.
- Вношу изменения.
- Защищаю изменения тестом.

Проверил работу исходной rake таски на разных объеиах данных:

* {"3.4.1":{"gc":"enabled","time":8.45,"gc_count":79,"memory":"58 MB"}} - small.json
* {"3.4.1":{"gc":"enabled","time":61.38,"gc_count":309,"memory":"94 MB"}} - medium.json
* {"3.4.1":{"gc":"enabled","time":572.29,"gc_count":763,"memory":"244 MB"}} - large.json

Для удобства работы с моделями добавил gem annotate.

Перед началом оптимизации написал тест который позволит контролировать что после изменений скрипт работает валидно(spec/services/reloader_spec.rb).

## Поиск точек роста
Проанализировал время выполнения с помощью 'ruby-prof', но не получил больших подробностей кроме того что большую часть времени
занимают подключения и раюота БД.

Решил проанализировать логи 'development.log'. Заметил огромное количество  SELECT запросов для City, Bus, Servise моделей
по каждой операции.

## Оптимизация
- Для удобства тестирования и работы со скриптом вынес логику в отдельный Reload сервис.
- Переписал скрипт в потоковом стиле, записывая трипы в базу на лету.
- Для очистки таблиц использовал TRUNCATE вместо .delete_all

## Метрика
* {"3.4.1":{"gc":"enabled","time":0.35,"gc_count":2,"memory":"14 MB"}} - small.json
* {"3.4.1":{"gc":"enabled","time":0.56,"gc_count":24,"memory":"14 MB"}} - medium.json
* {"3.4.1":{"gc":"enabled","time":2.6,"gc_count":243,"memory":"15 MB"}} - large.json
* {"3.4.1":{"gc":"enabled","time":24.16,"gc_count":586,"memory":"32 MB"}} - 1M.json
* {"3.4.1":{"gc":"enabled","time":222.63,"gc_count":5441,"memory":"18 MB"}} - 10M.json

Как результат, вложился в поставленную метрику в 1 мин для large.json - 2.6 сек.

